{"cells": [{"cell_type": "markdown", "metadata": {"id": "XvuVcBed2sMK"}, "source": "## CS4830 Project\n\n* Aniruddha (ME18B181)\n* Vasudev Gupta (ME18B182)\n* Shubham (ME18B183)"}, {"cell_type": "code", "execution_count": 1, "metadata": {"id": "7hyoX2081qxr"}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import (\n    StringIndexer,\n    IndexToString,\n    VectorAssembler,\n    OneHotEncoder,\n)\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline, PipelineModel"}, {"cell_type": "code", "execution_count": 2, "metadata": {"id": "Db1pGKVc1rHb"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"}, {"name": "stderr", "output_type": "stream", "text": "Ivy Default Cache set to: /root/.ivy2/cache\nThe jars for the packages stored in: /root/.ivy2/jars\ncom.johnsnowlabs.nlp#spark-nlp_2.11 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-31cd26bd-1ffa-4876-93c3-2643a5fa1ab0;1.0\n\tconfs: [default]\n\tfound com.johnsnowlabs.nlp#spark-nlp_2.11;2.4.5 in central\n\tfound com.typesafe#config;1.3.0 in central\n\tfound org.rocksdb#rocksdbjni;6.5.3 in central\n\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n\tfound com.amazonaws#aws-java-sdk-core;1.11.603 in central\n\tfound commons-logging#commons-logging;1.1.3 in central\n\tfound org.apache.httpcomponents#httpclient;4.5.9 in central\n\tfound org.apache.httpcomponents#httpcore;4.4.11 in central\n\tfound commons-codec#commons-codec;1.11 in central\n\tfound software.amazon.ion#ion-java;1.0.2 in central\n\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.6.7 in central\n\tfound joda-time#joda-time;2.8.1 in central\n\tfound com.amazonaws#aws-java-sdk-s3;1.11.603 in central\n\tfound com.amazonaws#aws-java-sdk-kms;1.11.603 in central\n\tfound com.amazonaws#jmespath-java;1.11.603 in central\n\tfound com.fasterxml.jackson.core#jackson-databind;2.6.7.2 in central\n\tfound com.fasterxml.jackson.core#jackson-annotations;2.6.0 in central\n\tfound com.fasterxml.jackson.core#jackson-core;2.6.7 in central\n\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n\tfound com.google.code.findbugs#annotations;3.0.1 in central\n\tfound net.jcip#jcip-annotations;1.0 in central\n\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n\tfound com.google.code.gson#gson;2.3 in central\n\tfound it.unimi.dsi#fastutil;7.0.12 in central\n\tfound org.projectlombok#lombok;1.16.8 in central\n\tfound org.slf4j#slf4j-api;1.7.21 in central\n\tfound com.navigamez#greex;1.0 in central\n\tfound dk.brics.automaton#automaton;1.11-8 in central\n\tfound org.json4s#json4s-ext_2.11;3.5.3 in central\n\tfound joda-time#joda-time;2.9.5 in central\n\tfound org.joda#joda-convert;1.8.1 in central\n\tfound org.tensorflow#tensorflow;1.15.0 in central\n\tfound org.tensorflow#libtensorflow;1.15.0 in central\n\tfound org.tensorflow#libtensorflow_jni;1.15.0 in central\n\tfound net.sf.trove4j#trove4j;3.0.3 in central\n:: resolution report :: resolve 990ms :: artifacts dl 28ms\n\t:: modules in use:\n\tcom.amazonaws#aws-java-sdk-core;1.11.603 from central in [default]\n\tcom.amazonaws#aws-java-sdk-kms;1.11.603 from central in [default]\n\tcom.amazonaws#aws-java-sdk-s3;1.11.603 from central in [default]\n\tcom.amazonaws#jmespath-java;1.11.603 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-annotations;2.6.0 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-core;2.6.7 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-databind;2.6.7.2 from central in [default]\n\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.6.7 from central in [default]\n\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n\tcom.google.code.gson#gson;2.3 from central in [default]\n\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n\tcom.johnsnowlabs.nlp#spark-nlp_2.11;2.4.5 from central in [default]\n\tcom.navigamez#greex;1.0 from central in [default]\n\tcom.typesafe#config;1.3.0 from central in [default]\n\tcommons-codec#commons-codec;1.11 from central in [default]\n\tcommons-logging#commons-logging;1.1.3 from central in [default]\n\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n\tjoda-time#joda-time;2.9.5 from central in [default]\n\tnet.jcip#jcip-annotations;1.0 from central in [default]\n\tnet.sf.trove4j#trove4j;3.0.3 from central in [default]\n\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n\torg.apache.httpcomponents#httpclient;4.5.9 from central in [default]\n\torg.apache.httpcomponents#httpcore;4.4.11 from central in [default]\n\torg.joda#joda-convert;1.8.1 from central in [default]\n\torg.json4s#json4s-ext_2.11;3.5.3 from central in [default]\n\torg.projectlombok#lombok;1.16.8 from central in [default]\n\torg.rocksdb#rocksdbjni;6.5.3 from central in [default]\n\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n\torg.tensorflow#libtensorflow;1.15.0 from central in [default]\n\torg.tensorflow#libtensorflow_jni;1.15.0 from central in [default]\n\torg.tensorflow#tensorflow;1.15.0 from central in [default]\n\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n\t:: evicted modules:\n\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n\tjoda-time#joda-time;2.8.1 by [joda-time#joda-time;2.9.5] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   38  |   0   |   0   |   2   ||   36  |   0   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-31cd26bd-1ffa-4876-93c3-2643a5fa1ab0\n\tconfs: [default]\n\t0 artifacts copied, 36 already retrieved (0kB/24ms)\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/05/15 11:44:51 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n22/05/15 11:44:51 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n22/05/15 11:44:51 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n22/05/15 11:44:51 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.11-2.4.5.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.typesafe_config-1.3.0.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.rocksdb_rocksdbjni-6.5.3.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.2.0.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-core-1.11.603.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-s3-1.11.603.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.navigamez_greex-1.0.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.json4s_json4s-ext_2.11-3.5.3.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.tensorflow_tensorflow-1.15.0.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/net.sf.trove4j_trove4j-3.0.3.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.9.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/software.amazon.ion_ion-java-1.0.2.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.6.7.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.11.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/commons-codec_commons-codec-1.11.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-kms-1.11.603.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.amazonaws_jmespath-java-1.11.603.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.6.7.2.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.6.0.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.6.7.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.code.findbugs_annotations-3.0.1.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.0.0-beta-3.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-3.0.0-beta-3.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.21.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/net.jcip_jcip-annotations-1.0.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.1.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.code.gson_gson-2.3.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/joda-time_joda-time-2.9.5.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.joda_joda-convert-1.8.1.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.tensorflow_libtensorflow-1.15.0.jar added multiple times to distributed cache.\n22/05/15 11:44:57 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.tensorflow_libtensorflow_jni-1.15.0.jar added multiple times to distributed cache.\n"}], "source": "spark = (\n    SparkSession.builder.appName(\"CS4830_project\")\n    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\")\n    .getOrCreate()\n)"}, {"cell_type": "code", "execution_count": 3, "metadata": {"id": "6AEG82lH1rJX"}, "outputs": [], "source": "DATA_PATH = \"gs://big-data-cs4830/project/trainingdatanyc.csv/*.csv\""}, {"cell_type": "markdown", "metadata": {"id": "44VimJBv11e5"}, "source": "## Data Exploration Steps"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "b-IuFan21rLv"}, "outputs": [], "source": "df = spark.read.option(\"header\", \"true\").csv(DATA_PATH)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Yiq21aiQ1rNx"}, "outputs": [], "source": "df = df.filter(col(\"Violation Precinct\").isNotNull())\ndf.select(\"Violation Precinct\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "pGOQfzjh1rQK"}, "outputs": [], "source": "df.select(\"Violation Precinct\").distinct().sort(\"Violation Precinct\").show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "BDdeRBra1rSa"}, "outputs": [], "source": "df.printSchema()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "yVYN37bj1rUW"}, "outputs": [], "source": "df.select(\n    [count(when(isnan(col) | isnull(col), col)).alias(col) for col in df.columns]\n).show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "fUwX7ZvA1rWa"}, "outputs": [], "source": "# removed due to presence of many null values\ncols_to_drop = [\n    \"Time First Observed\",\n    \"Intersecting Street\",\n    \"Law Section\",\n    \"Violation Legal Code\",\n    \"To Hours In Effect\",\n    \"Unregistered Vehicle?\",\n    \"Meter Number\",\n    \"Violation Description\",\n    \"No Standing or Stopping Violation\",\n    \"Hydrant Violation\",\n    \"Double Parking Violation\",\n    \"Latitude\",\n    \"Longitude\",\n    \"Community Board\",\n    \"Community Council\",\n    \"Census Tract\",\n    \"BIN\",\n    \"BBL\",\n    \"NTA\",\n]\ndf = df.select([col for col in df.columns if col not in cols_to_drop])"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "aDnCgbkE1rZz"}, "outputs": [], "source": "# handling null values\ndf = df.na.fill(\"NULL\")"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "k-pPva2W1rcG"}, "outputs": [], "source": "for col in df.columns:\n    print(col, df.select(col).distinct().count())"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "r0KEhnC21reK"}, "outputs": [], "source": "input_columns = [\n    \"Feet From Curb\",\n    \"Violation In Front Of Or Opposite\",\n    \"Issuing Agency\",\n    \"Violation County\",\n    \"Plate Type\",\n    \"Violation Code\",\n    \"Registration State\",\n    \"Issuer Squad\",\n]\ndf = df.select(input_columns + [\"Violation Precinct\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "ZBhQYEqL1rgz"}, "outputs": [], "source": "df.printSchema()"}, {"cell_type": "markdown", "metadata": {"id": "OJNNv68s1_Pb"}, "source": "## Exporting code for production"}, {"cell_type": "code", "execution_count": 4, "metadata": {"id": "lVMYCSjq1ri9"}, "outputs": [], "source": "INPUT_COLUMNS = [\n    \"Feet From Curb\",\n    \"Violation In Front Of Or Opposite\",\n    \"Issuing Agency\",\n    \"Violation County\",\n    \"Plate Type\",\n    \"Violation Code\",\n    \"Registration State\",\n    \"Issuer Squad\",\n]\nTARGET_COLUMN = \"Violation Precinct\""}, {"cell_type": "code", "execution_count": 5, "metadata": {"id": "8M2MgNjK2A8h"}, "outputs": [], "source": "def read_and_prepare_data(path):\n    df = spark.read.option(\"header\", \"true\").csv(path)\n    df = df.filter(col(TARGET_COLUMN).isNotNull())\n\n    df = df.na.fill(\"NULL\")\n    df = df.select(INPUT_COLUMNS + [TARGET_COLUMN])\n\n    return df"}, {"cell_type": "markdown", "metadata": {"id": "Gs6qKryU2E8x"}, "source": "## Preparing data for training & inference"}, {"cell_type": "code", "execution_count": 6, "metadata": {"id": "QIt10HNX2A-l"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- Feet From Curb: string (nullable = false)\n |-- Violation In Front Of Or Opposite: string (nullable = false)\n |-- Issuing Agency: string (nullable = false)\n |-- Violation County: string (nullable = false)\n |-- Plate Type: string (nullable = false)\n |-- Violation Code: string (nullable = false)\n |-- Registration State: string (nullable = false)\n |-- Issuer Squad: string (nullable = false)\n |-- Violation Precinct: string (nullable = false)\n\n"}], "source": "data = read_and_prepare_data(DATA_PATH)\ndata.printSchema()"}, {"cell_type": "code", "execution_count": 9, "metadata": {"id": "P3vB6io02BA-"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 9:======================================================>  (48 + 2) / 50]\r"}, {"name": "stdout", "output_type": "stream", "text": "{'train_size': 224367, 'val_size': 22468}\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "tr_data, val_data, _ = data.randomSplit([0.01, 0.001, 0.989], seed=42)\nprint({\"train_size\": tr_data.count(), \"val_size\": val_data.count()})"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "kbmKZO8X2BDX"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 2:>                                                          (0 + 1) / 1]\r"}], "source": "# # just for testing purposes\n# _, tr_data = data.randomSplit([0.999, 0.001], seed=42)\n# val_data = tr_data\n# tr_data.count()"}, {"cell_type": "markdown", "metadata": {"id": "txfaYQZ72KL5"}, "source": "## Setting up model pipeline"}, {"cell_type": "code", "execution_count": 8, "metadata": {"id": "Uuqsk9IS2BF4"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "labels: ['0', '19', '18', '14', '1', '114', '13', '109', '17', '20', '84', '70', '115', '61', '112', '6', '66', '10', '52', '103', '108', '9', '5', '90', '24', '68', '110', '62', '104', '46', '78', '102', '43', '49', '107', '34', '94', '23', '7', '47', '77', '33', '44', '67', '72', '48', '40', '45', '88', '106', '105', '79', '28', '50', '83', '63', '75', '60', '71', '30', '25', '26', '32', '41', '111', '76', '73', '42', '69', '120', '113', '122', '81', '100', '101', '121', '123', '27', '22', '2', '11', '15', '12', '3', '65', '87', '127', '21', '4', '8', '119', '16', '29', '31', '39', '56', '74', '98', '116', '118', '133', '170', '276', '35', '36', '367', '37', '408', '428', '501', '54', '57', '58', '64', '668', '80', '806', '85', '89', '91', '923', '96', '99']\n"}], "source": "labelIndexer = StringIndexer(inputCol=TARGET_COLUMN, outputCol=\"label\")\\\n    .setHandleInvalid(\"skip\")\\\n    .fit(tr_data)\nprint(\"labels:\", labelIndexer.labels)"}, {"cell_type": "code", "execution_count": 9, "metadata": {"id": "3WS6JtoG2BIT"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "feature_indexers = [\n    StringIndexer(inputCol=col, outputCol=col + \"_index\").setHandleInvalid(\"keep\") for col in INPUT_COLUMNS\n]\nfeature_pipeline = Pipeline(stages=feature_indexers).fit(tr_data)"}, {"cell_type": "code", "execution_count": 10, "metadata": {"id": "HX9b6Kmw2BKT"}, "outputs": [], "source": "OHE = OneHotEncoder(\n    inputCols=[col + \"_index\" for col in INPUT_COLUMNS],\n    outputCols=[col + \"_onehot\" for col in INPUT_COLUMNS],\n)"}, {"cell_type": "code", "execution_count": 11, "metadata": {"id": "zed6Nkg82BMf"}, "outputs": [], "source": "assembler = VectorAssembler(\n    inputCols=[col + \"_onehot\" for col in INPUT_COLUMNS],\n    outputCol=\"features\",\n)"}, {"cell_type": "code", "execution_count": 12, "metadata": {"id": "vZbHBQoW2BO0"}, "outputs": [], "source": "model = LogisticRegression(\n    featuresCol=\"features\", labelCol=\"label\", predictionCol=\"class\"\n)\nindex_to_string = IndexToString(\n    inputCol=\"class\", outputCol=\"prediction\", labels=labelIndexer.labels\n)"}, {"cell_type": "code", "execution_count": 13, "metadata": {"id": "Jqx50PAF2BRG"}, "outputs": [], "source": "stages = [\n    labelIndexer,\n    feature_pipeline,\n    OHE,\n    assembler,\n    model,\n    index_to_string,\n]\npipeline = Pipeline(stages=stages)"}, {"cell_type": "code", "execution_count": 14, "metadata": {"id": "UtX5imLG2T1q"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "22/05/15 11:59:36 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n22/05/15 11:59:36 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n                                                                                \r"}], "source": "pipeline = pipeline.fit(tr_data)"}, {"cell_type": "code", "execution_count": 15, "metadata": {"id": "pBgdWCng2T4C"}, "outputs": [], "source": "MODEL_PATH = \"gs://big-data-cs4830/project/final_model\""}, {"cell_type": "code", "execution_count": 17, "metadata": {"id": "odlSkcOF2T6c"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "pipeline.save(MODEL_PATH)"}, {"cell_type": "markdown", "metadata": {"id": "Otl2nQXZ2XPr"}, "source": "## Using model for inference"}, {"cell_type": "code", "execution_count": 18, "metadata": {"id": "_ux_mo202T8p"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# running the model directly for testing\npipeline = PipelineModel.load(MODEL_PATH)"}, {"cell_type": "code", "execution_count": 19, "metadata": {"id": "QE6yuzyj2BTQ"}, "outputs": [], "source": "accuracy_metric = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"class\", metricName=\"accuracy\"\n)\nf1_metric = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"class\", metricName=\"f1\"\n)"}, {"cell_type": "code", "execution_count": 22, "metadata": {"id": "lB0ZtAdM2Zvp"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 385:=====================================================> (49 + 1) / 50]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+-----+\n|class|label|\n+-----+-----+\n| 39.0| 29.0|\n| 39.0| 39.0|\n| 29.0| 18.0|\n| 32.0| 42.0|\n| 29.0| 46.0|\n| 29.0| 63.0|\n| 29.0| 32.0|\n| 39.0| 63.0|\n| 39.0| 18.0|\n| 46.0| 39.0|\n| 63.0| 63.0|\n| 63.0| 32.0|\n| 29.0| 39.0|\n| 29.0| 67.0|\n| 29.0| 29.0|\n| 29.0| 45.0|\n| 29.0| 33.0|\n| 29.0| 18.0|\n| 29.0| 42.0|\n| 29.0| 45.0|\n+-----+-----+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "tr_pred = pipeline.transform(tr_data.limit(20000)).select(\"class\", \"label\")\ntr_pred.show()"}, {"cell_type": "code", "execution_count": 24, "metadata": {"id": "2hg-929Z2Zxy"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "(0.3599, 0.3346105074712877)"}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": "accuracy_metric.evaluate(tr_pred), f1_metric.evaluate(tr_pred)"}, {"cell_type": "code", "execution_count": 20, "metadata": {"id": "25v1D2Jk2Z0P"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 380:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+-----+\n|class|label|\n+-----+-----+\n| 29.0| 42.0|\n| 32.0| 29.0|\n| 39.0| 33.0|\n| 39.0| 53.0|\n| 10.0| 10.0|\n| 58.0| 16.0|\n| 58.0| 72.0|\n| 43.0| 54.0|\n| 43.0| 65.0|\n|  4.0| 35.0|\n|  1.0| 15.0|\n| 24.0| 24.0|\n|  1.0| 60.0|\n|  3.0|  8.0|\n| 35.0| 61.0|\n| 70.0| 28.0|\n| 12.0|  5.0|\n| 12.0|  7.0|\n| 44.0| 58.0|\n| 27.0| 11.0|\n+-----+-----+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "val_pred = pipeline.transform(val_data).select(\"class\", \"label\")\nval_pred.show()"}, {"cell_type": "code", "execution_count": 21, "metadata": {"id": "Ovj7VBJU2Z2d"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "(0.36024927665257067, 0.33193978290893916)"}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": "accuracy_metric.evaluate(val_pred), f1_metric.evaluate(val_pred)"}, {"cell_type": "markdown", "metadata": {"id": "lWgGKhdp2fhM"}, "source": "### Kafka Producer"}, {"cell_type": "code", "execution_count": 25, "metadata": {"id": "GeVGIqM92Z4o"}, "outputs": [], "source": "# TODO: change following for demo\nREAL_TIME_DATA_PATH = DATA_PATH\nBROKER = \"10.128.0.40:9092\"\nTOPIC = \"CS4830-project\"\nLIMIT = 150"}, {"cell_type": "code", "execution_count": 26, "metadata": {"id": "dCUkjVsd2Z7M"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m"}], "source": "!pip3 install -U -q kafka-python\n\nimport os\n\nos.environ[\n    \"PYSPARK_SUBMIT_ARGS\"\n] = \"--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\""}, {"cell_type": "code", "execution_count": 29, "metadata": {"id": "zGCG3KPD2Z9a"}, "outputs": [], "source": "import json\nimport time\nfrom kafka import KafkaProducer\nfrom tqdm.auto import tqdm"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "RvNxNHp82Z_o"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "real_time_df = read_and_prepare_data(REAL_TIME_DATA_PATH)"}, {"cell_type": "code", "execution_count": 30, "metadata": {"id": "DiCxQr9G2kJ6"}, "outputs": [], "source": "# data file should be small otherwise one needs to allocate bigger cluster\nreal_time_df = real_time_df.limit(LIMIT)"}, {"cell_type": "code", "execution_count": 31, "metadata": {"id": "n3Oe4l1-2kMN"}, "outputs": [], "source": "producer = KafkaProducer(\n    bootstrap_servers=[BROKER], value_serializer=lambda x: json.dumps(x).encode(\"utf-8\")\n)"}, {"cell_type": "code", "execution_count": 32, "metadata": {"id": "hFEBxSbo2kOp"}, "outputs": [], "source": "pandas_df = real_time_df.toPandas()"}, {"cell_type": "code", "execution_count": 34, "metadata": {"id": "e4UMqDUA2kRQ"}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "f871dc1b8684431489a2ab2f339ac114", "version_major": 2, "version_minor": 0}, "text/plain": "0it [00:00, ?it/s]"}, "metadata": {}, "output_type": "display_data"}], "source": "for index, row in tqdm(pandas_df.iterrows()):\n    payload = \",\".join(str(x) for x in row.to_dict().values())\n    producer.send(TOPIC, value=\",\" + payload + \",\")\n    producer.flush()\n    time.sleep(0.1)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Vs82gT9V2kYD"}, "outputs": [], "source": ""}], "metadata": {"colab": {"collapsed_sections": [], "name": "project.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}}, "nbformat": 4, "nbformat_minor": 4}